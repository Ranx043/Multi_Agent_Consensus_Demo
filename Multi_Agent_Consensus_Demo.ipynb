{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multi-Agent Consensus Voting System\n",
        "## Proof of Concept for Vedic Astrology Platform\n",
        "\n",
        "This notebook demonstrates the consensus voting mechanism for reconciling interpretations from multiple specialized LLM agents.\n",
        "\n",
        "**Key Features:**\n",
        "- Weighted voting by domain type (career vs marriage have different weights)\n",
        "- Confidence-adjusted contributions\n",
        "- Conflict detection (threshold-based)\n",
        "- Multiple resolution strategies\n",
        "- Clean JSON output ready for API\n",
        "\n",
        "---\n",
        "*Author: Randhy Paul | December 2025*"
      ],
      "metadata": {
        "id": "header"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data Structures"
      ],
      "metadata": {
        "id": "section1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict\n",
        "from enum import Enum\n",
        "import json\n",
        "\n",
        "class ResolutionStrategy(Enum):\n",
        "    UNANIMOUS = \"unanimous\"\n",
        "    WEIGHTED_MAJORITY = \"weighted_majority\"\n",
        "    NUANCE_ARBITRATION = \"nuance_arbitration\"\n",
        "    MATH_OVERRIDE = \"math_override\"\n",
        "\n",
        "@dataclass\n",
        "class AgentResponse:\n",
        "    \"\"\"Structured response from each specialized agent.\"\"\"\n",
        "    agent_id: str\n",
        "    domain: str\n",
        "    interpretation: str\n",
        "    score: float  # 0-100\n",
        "    confidence: float  # 0-1\n",
        "    supporting_factors: List[str]\n",
        "    contradicting_factors: List[str]\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"agent_id\": self.agent_id,\n",
        "            \"domain\": self.domain,\n",
        "            \"score\": self.score,\n",
        "            \"confidence\": self.confidence\n",
        "        }\n",
        "\n",
        "@dataclass\n",
        "class ConsensusResult:\n",
        "    \"\"\"Final consensus output.\"\"\"\n",
        "    domain: str\n",
        "    final_score: float\n",
        "    final_interpretation: str\n",
        "    confidence: float\n",
        "    agreement_level: str\n",
        "    strategy_used: ResolutionStrategy\n",
        "    conflicts_detected: int\n",
        "    conflicts_resolved: int\n",
        "    agent_contributions: Dict[str, float]\n",
        "\n",
        "    def to_dict(self) -> dict:\n",
        "        return {\n",
        "            \"domain\": self.domain,\n",
        "            \"final_score\": round(self.final_score, 2),\n",
        "            \"confidence\": round(self.confidence, 3),\n",
        "            \"agreement_level\": self.agreement_level,\n",
        "            \"strategy_used\": self.strategy_used.value,\n",
        "            \"conflicts_detected\": self.conflicts_detected,\n",
        "            \"conflicts_resolved\": self.conflicts_resolved\n",
        "        }\n",
        "\n",
        "print(\"Data structures loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Consensus Engine"
      ],
      "metadata": {
        "id": "section2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConsensusEngine:\n",
        "    \"\"\"\n",
        "    Multi-agent consensus with weighted voting and conflict resolution.\n",
        "    \"\"\"\n",
        "\n",
        "    # Agent weights vary by domain\n",
        "    DOMAIN_WEIGHTS = {\n",
        "        \"career\": {\n",
        "            \"integration_specialist\": 0.30,\n",
        "            \"mathematics_validator\": 0.15,\n",
        "            \"risk_assessor\": 0.25,\n",
        "            \"nuance_specialist\": 0.30\n",
        "        },\n",
        "        \"marriage\": {\n",
        "            \"integration_specialist\": 0.25,\n",
        "            \"mathematics_validator\": 0.10,\n",
        "            \"risk_assessor\": 0.25,\n",
        "            \"nuance_specialist\": 0.40  # Higher for marriage domain\n",
        "        }\n",
        "    }\n",
        "\n",
        "    CONFLICT_THRESHOLD = 15.0\n",
        "\n",
        "    def __init__(self):\n",
        "        self.resolution_log = []\n",
        "\n",
        "    def calculate_consensus(self, responses: List[AgentResponse], domain: str) -> ConsensusResult:\n",
        "        weights = self.DOMAIN_WEIGHTS.get(domain, self.DOMAIN_WEIGHTS[\"career\"])\n",
        "\n",
        "        # Step 1: Calculate weighted scores\n",
        "        weighted_sum = 0.0\n",
        "        weight_total = 0.0\n",
        "\n",
        "        for response in responses:\n",
        "            agent_weight = weights.get(response.agent_id, 0.25)\n",
        "            effective_weight = agent_weight * response.confidence\n",
        "            weighted_sum += response.score * effective_weight\n",
        "            weight_total += effective_weight\n",
        "\n",
        "        initial_score = weighted_sum / weight_total if weight_total > 0 else 50.0\n",
        "\n",
        "        # Step 2: Detect conflicts\n",
        "        conflicts = self._detect_conflicts(responses, initial_score)\n",
        "\n",
        "        # Step 3: Resolve conflicts\n",
        "        if conflicts:\n",
        "            final_score, strategy = self._resolve_conflicts(responses, conflicts, initial_score, domain)\n",
        "        else:\n",
        "            final_score = initial_score\n",
        "            strategy = ResolutionStrategy.UNANIMOUS\n",
        "\n",
        "        # Step 4: Calculate agreement level\n",
        "        score_range = max(r.score for r in responses) - min(r.score for r in responses)\n",
        "        agreement_level = \"high\" if score_range <= 10 else (\"medium\" if score_range <= 20 else \"low\")\n",
        "\n",
        "        # Step 5: Final confidence\n",
        "        avg_confidence = sum(r.confidence for r in responses) / len(responses)\n",
        "        agreement_bonus = 0.1 if agreement_level == \"high\" else (-0.1 if agreement_level == \"low\" else 0)\n",
        "        final_confidence = min(1.0, avg_confidence + agreement_bonus)\n",
        "\n",
        "        return ConsensusResult(\n",
        "            domain=domain,\n",
        "            final_score=final_score,\n",
        "            final_interpretation=self._generate_interpretation(final_score, agreement_level, domain),\n",
        "            confidence=final_confidence,\n",
        "            agreement_level=agreement_level,\n",
        "            strategy_used=strategy,\n",
        "            conflicts_detected=len(conflicts),\n",
        "            conflicts_resolved=len(conflicts),\n",
        "            agent_contributions={r.agent_id: weights.get(r.agent_id, 0.25) * r.confidence for r in responses}\n",
        "        )\n",
        "\n",
        "    def _detect_conflicts(self, responses, mean_score):\n",
        "        return [r for r in responses if abs(r.score - mean_score) > self.CONFLICT_THRESHOLD]\n",
        "\n",
        "    def _resolve_conflicts(self, responses, conflicts, initial_score, domain):\n",
        "        # Check for nuance arbitration in marriage domain\n",
        "        nuance_conflict = next((c for c in conflicts if c.agent_id == \"nuance_specialist\"), None)\n",
        "        if nuance_conflict and domain == \"marriage\":\n",
        "            self.resolution_log.append({\"strategy\": \"NUANCE_ARBITRATION\", \"reason\": \"Marriage domain prioritizes nuance\"})\n",
        "            return 0.5 * nuance_conflict.score + 0.5 * initial_score, ResolutionStrategy.NUANCE_ARBITRATION\n",
        "\n",
        "        # Default: weighted majority using top 3 by confidence\n",
        "        sorted_responses = sorted(responses, key=lambda r: r.confidence, reverse=True)[:3]\n",
        "        recalc_sum = sum(r.score * r.confidence for r in sorted_responses)\n",
        "        recalc_weight = sum(r.confidence for r in sorted_responses)\n",
        "        self.resolution_log.append({\"strategy\": \"WEIGHTED_MAJORITY\", \"reason\": \"Using top 3 agents by confidence\"})\n",
        "        return recalc_sum / recalc_weight, ResolutionStrategy.WEIGHTED_MAJORITY\n",
        "\n",
        "    def _generate_interpretation(self, score, agreement, domain):\n",
        "        tier = \"excellent\" if score >= 80 else (\"good\" if score >= 65 else (\"moderate\" if score >= 50 else \"challenging\"))\n",
        "        return f\"{domain.title()} shows {tier} potential (score: {score:.1f}/100). Agreement: {agreement}.\"\n",
        "\n",
        "print(\"Consensus Engine loaded!\")"
      ],
      "metadata": {
        "id": "consensus_engine"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Test Cases"
      ],
      "metadata": {
        "id": "section3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_career_responses():\n",
        "    \"\"\"Simulated responses for career domain - HIGH AGREEMENT expected\"\"\"\n",
        "    return [\n",
        "        AgentResponse(\"integration_specialist\", \"career\", \"Strong 10th house\", 78.5, 0.87, [\"Jupiter aspects 10th\"], []),\n",
        "        AgentResponse(\"mathematics_validator\", \"career\", \"SAV 32/48\", 72.0, 0.95, [\"SAV above average\"], []),\n",
        "        AgentResponse(\"risk_assessor\", \"career\", \"Low risk\", 81.0, 0.82, [\"No Kemadruma\"], []),\n",
        "        AgentResponse(\"nuance_specialist\", \"career\", \"Steady rise\", 75.0, 0.79, [\"Neecha Bhanga\"], [])\n",
        "    ]\n",
        "\n",
        "def get_marriage_responses():\n",
        "    \"\"\"Simulated responses for marriage domain - CONFLICT expected (nuance diverges)\"\"\"\n",
        "    return [\n",
        "        AgentResponse(\"integration_specialist\", \"marriage\", \"Venus strong\", 70.0, 0.85, [\"Venus in own sign\"], []),\n",
        "        AgentResponse(\"mathematics_validator\", \"marriage\", \"SAV 28/48\", 58.0, 0.92, [\"Average SAV\"], []),\n",
        "        AgentResponse(\"risk_assessor\", \"marriage\", \"Manglik cancelled\", 62.0, 0.88, [\"Cancellation applies\"], []),\n",
        "        AgentResponse(\"nuance_specialist\", \"marriage\", \"D9 excellent\", 88.0, 0.75, [\"D9 Venus exalted\"], [])  # DIVERGENT!\n",
        "    ]\n",
        "\n",
        "print(\"Test cases ready!\")"
      ],
      "metadata": {
        "id": "test_cases"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Run Tests"
      ],
      "metadata": {
        "id": "section4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "engine = ConsensusEngine()\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"TEST 1: CAREER DOMAIN (Expected: High Agreement)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "career_responses = get_career_responses()\n",
        "print(\"\\nAgent Responses:\")\n",
        "for r in career_responses:\n",
        "    print(f\"  {r.agent_id}: score={r.score}, confidence={r.confidence}\")\n",
        "\n",
        "result1 = engine.calculate_consensus(career_responses, \"career\")\n",
        "print(\"\\nConsensus Result:\")\n",
        "print(json.dumps(result1.to_dict(), indent=2))"
      ],
      "metadata": {
        "id": "test1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TEST 2: MARRIAGE DOMAIN (Expected: Conflict + Resolution)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "marriage_responses = get_marriage_responses()\n",
        "print(\"\\nAgent Responses:\")\n",
        "for r in marriage_responses:\n",
        "    print(f\"  {r.agent_id}: score={r.score}, confidence={r.confidence}\")\n",
        "\n",
        "result2 = engine.calculate_consensus(marriage_responses, \"marriage\")\n",
        "print(\"\\nConsensus Result:\")\n",
        "print(json.dumps(result2.to_dict(), indent=2))\n",
        "\n",
        "if engine.resolution_log:\n",
        "    print(\"\\nResolution Log:\")\n",
        "    for log in engine.resolution_log:\n",
        "        print(f\"  Strategy: {log['strategy']}\")\n",
        "        print(f\"  Reason: {log['reason']}\")"
      ],
      "metadata": {
        "id": "test2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Summary\n",
        "\n",
        "This prototype demonstrates:\n",
        "\n",
        "| Feature | Status |\n",
        "|---------|--------|\n",
        "| Weighted voting by domain | OK |\n",
        "| Confidence-adjusted contributions | OK |\n",
        "| Conflict detection | OK |\n",
        "| Resolution strategies | OK |\n",
        "| Agreement level calculation | OK |\n",
        "| JSON output for API | OK |\n",
        "\n",
        "**Ready for production implementation with real LLM agents.**"
      ],
      "metadata": {
        "id": "summary"
      }
    }
  ]
}